version: 2.1

orbs:
  aws-ecr: circleci/aws-ecr@7.0.0

aws_docker_image: &aws_docker_image
  docker:
    - image: amazon/aws-cli

python_docker_image: &python_docker_image
  docker:
    - image: python:3.7.3-stretch

commands:
  destroy-environment:
    description: Destroy vpc and eks cloudformation stacks given a workflow ID.    
    steps:
      - run:
          name: Destroy environment
          when: on_fail
          command: |              
              aws cloudformation delete-stack --stack-name eksctl-${ENV_NAME}-cluster-fargate
              aws cloudformation delete-stack --stack-name eksctl-${ENV_NAME}-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller 
              aws cloudformation wait stack-delete-complete --stack-name eksctl-${ENV_NAME}-nodegroup-${ENV_NAME}-worker

              # Delete fargate profile associated with cluster
              # eksctl delete fargateprofile --cluster ${ENV_NAME}-cluster --name uda-capstone-project
              
              aws cloudformation delete-stack --stack-name eksctl-${ENV_NAME}-cluster-cluster              
              # Detach and delete policy
              aws iam detach-role-policy --policy-arn arn:aws:iam::246528985509:policy/AWSLoadBalancerControllerIAMPolicy --role-name AmazonEKSLoadBalancerControllerRole
              aws iam delete-policy --policy-arn arn:aws:iam::246528985509:policy/AWSLoadBalancerControllerIAMPolicy
              aws iam delete-role --role-name AmazonEKSLoadBalancerControllerRole    

jobs:

    build-test-lint:
      <<: *python_docker_image
      working_directory: ~/repo
      steps:
        - checkout
        - restore_cache:
            keys:
              - v1-dependencies-{{ checksum "./app/requirements.txt" }}
              # fallback to using the latest cache if no exact match is found
              - v1-dependencies-
        - run:
            name: set up python
            command: |
                cd ./app
                make setup
        - run:
            name: install dependencies
            command: |
                cd ./app
                make install
        - save_cache:
            paths:
                - ./.devops
            key: v1-dependencies-{{ checksum "./app/requirements.txt" }}
        - run:
            name: run test
            command: |
                cd ./app
                make test
        - run:
            name: run lint
            command: |
              cd ./app
              source .devops/bin/activate
              make lint
  
    deploy-aws-eks:
      <<: *aws_docker_image
      steps:
        - checkout
        - run:  
            name: Install tar and gzip
            command: |
              yum install -y tar gzip
        - run:
            name: install eksctl
            command: |
                curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
                mv /tmp/eksctl /usr/local/bin
        - run:
            name: install kubectl
            command: |
              curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" &&\
              curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256" &&\
              echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check &&
              chmod +x kubectl
              mv ./kubectl /usr/local/bin/kubectl
              kubectl version --short --client
        - run:
            name: create eks cluster and worker nodes
            command: |

              #Verify if the cluster already exist cluster
              export VALIDATE=$(aws eks describe-cluster --name ${ENV_NAME}-cluster --query 'cluster.name')
              echo "$VALIDATE" > ~/validate
              if cat ~/validate | grep "${ENV_NAME}-cluster"
              then
                  echo "Cluster ${ENV_NAME}-cluster already exist. Skipping job"
                  circleci-agent step halt
                  exit 0
              else
                  echo "Attempting to create the cluster $ENV_NAME-cluster"
              fi

              cd .circleci/scripts
              chmod +x deploy-cluster.sh
              ./deploy-cluster.sh ${ENV_NAME}
            no_output_timeout: 60m
        - run:
            name: install openSSL
            command: yum install -y openssl
        - run:
            name: install helm
            command: |
              curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh
              chmod 700 get_helm.sh
              ./get_helm.sh
        - run: 
            name: create IAM OIDC
            command: eksctl utils associate-iam-oidc-provider --region us-east-1 --cluster ${ENV_NAME}-cluster --approve
        - run:
            name: create load balancer policy
            command: |           
              cd .circleci/scripts
              chmod +x create-alb-policy.sh
              ./create-alb-policy.sh ${ENV_NAME} AmazonEKSLoadBalancerControllerRole ${AWS_ACCOUNT}
        - run:
            name: setup ALB
            command: |
              cd .circleci/scripts
              chmod +x deploy-alb.sh
              export VPC=$(aws ec2 describe-vpcs --region us-east-1 --filters "Name=tag-value,Values=eksctl-${ENV_NAME}-cluster-cluster/VPC" --query 'Vpcs[*].VpcId' --output text)
              
              ./deploy-alb.sh ${ENV_NAME} ${AWS_ACCOUNT} $VPC
        - run:
            name: create fargarte profile
            command: |
              eksctl create fargateprofile \
                    --cluster ${ENV_NAME}-cluster \
                    --region ${AWS_DEFAULT_REGION} \
                    --name uda-capstone-project \
                    --namespace ${ENV_NAME}
            no_output_timeout: 60m
        - destroy-environment

    deploy-service-in-eks:
      <<: *aws_docker_image
      steps:
        - checkout
        - run:  
            name: Install tar and gzip
            command: |
              yum install -y tar gzip
        - run:
            name: install kubectl
            command: |
              curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" &&\
              curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256" &&\
              echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check &&\
              chmod +x kubectl
              mv ./kubectl /usr/local/bin/kubectl
              kubectl version --short --client
        - run:
            name: Install envsubst
            command: yum -y install gettext
        - run:
            name: deploy pods
            command: |
              aws eks update-kubeconfig --name ${ENV_NAME}-cluster --region us-east-1
              
              # Create fargate profile namespace
              kubectl create namespace ${ENV_NAME}

              cd .circleci/files

              export ENV=${ENV_NAME}
              export ECR=${ECR_URI}

              # Create deployment
              envsubst < deployment.yml | kubectl apply -f -
              kubectl get deployments

              # Wait until pod is ready
              sleep 5m
              
              # Deploy service
              envsubst < service.yml | kubectl apply -f -

              # Verify service
              kubectl get svc uda-capstone-project -n ${ENV_NAME}

              # kubectl port-forward $POD_NAME --address 0.0.0.0 8000:80
              echo ${CIRCLE_WORKFLOW_ID:0:7} >> OldDeploymentId.txt
            no_output_timeout: 60m
        - persist_to_workspace:
            root: ~/
            paths:
              - project/OldDeploymentId.txt
workflows:
  deploy-application:
    jobs:
      - build-test-lint:
          filters:
            branches:
              only: [main]
      - deploy-aws-eks:
          filters:
            branches:
              only: [main, deploy-infrastructure]
      - aws-ecr/build-and-push-image:
          requires: [deploy-aws-eks]
          filters:
            branches:
              only: [main]
          account-url: ECR_URI
          aws-access-key-id: AWS_ACCESS_KEY_ID
          aws-secret-access-key: AWS_SECRET_ACCESS_KEY
          region: AWS_DEFAULT_REGION
          create-repo: true
          dockerfile: Dockerfile
          path: ./app
          repo: uda-capstone-project
          tag: latest
      - deploy-service-in-eks:
          requires: [aws-ecr/build-and-push-image]
          filters:
            branches:
              only: [main]